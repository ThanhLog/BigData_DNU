from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, sum, abs
from pyspark.sql.types import DoubleType
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml import Pipeline
import time

# ğŸ”¥ Táº¡o SparkSession vá»›i Spark UI (cá»•ng 4040)
spark = SparkSession.builder \
    .appName("COVID-19 Prediction with Spark UI") \
    .config("spark.ui.port", "4041") \
    .getOrCreate()

# ğŸš€ Giáº£m má»©c log Ä‘á»ƒ dá»… quan sÃ¡t trÃªn Spark UI
spark.sparkContext.setLogLevel("ERROR")

# â³ Theo dÃµi thá»i gian thá»±c thi
start_time = time.time()

print("ğŸ“¥ Äá»c dá»¯ liá»‡u...")
df = spark.read.csv(r"D:\Big_data\usa_county_wise_cleaned.csv", header=True, inferSchema=True)

# ğŸ›  Chuyá»ƒn Ä‘á»•i kiá»ƒu dá»¯ liá»‡u
df = df.withColumn("Confirmed", col("Confirmed").cast(DoubleType())) \
       .withColumn("Deaths", col("Deaths").cast(DoubleType())) \
       .withColumn("Lat", col("Lat").cast(DoubleType())) \
       .withColumn("Long_", col("Long_").cast(DoubleType()))

# ğŸ§¹ Xá»­ lÃ½ dá»¯ liá»‡u bá»‹ thiáº¿u
df = df.dropna(subset=["Confirmed", "Deaths", "Lat", "Long_"])

# ğŸ¯ Chuáº©n bá»‹ Ä‘áº·c trÆ°ng nhÃ³m theo bang
def prepare_features(dataframe):
    print("ğŸ”„ TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng...")
    grouped_df = dataframe.groupBy("Province_State").agg(
        avg("Confirmed").alias("Avg_Confirmed"),
        sum("Confirmed").alias("Total_Confirmed"),
        avg("Deaths").alias("Avg_Deaths"),
        sum("Deaths").alias("Total_Deaths"),
        avg("Lat").alias("Avg_Latitude"),
        avg("Long_").alias("Avg_Longitude")
    )
    return grouped_df

# ğŸ”„ Chuáº©n bá»‹ dá»¯ liá»‡u huáº¥n luyá»‡n
def prepare_model_data(dataframe):
    feature_cols = ["Avg_Latitude", "Avg_Longitude", "Avg_Confirmed", "Total_Confirmed", "Avg_Deaths", "Total_Deaths"]

    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
    scaler = StandardScaler(inputCol="features", outputCol="scaled_features", withStd=True, withMean=True)

    dataframe = dataframe.withColumn("label", col("Total_Confirmed"))
    return dataframe, assembler, scaler

# ğŸŒ³ XÃ¢y dá»±ng mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n
def build_prediction_model(assembler, scaler):
    print("ğŸ“¡ XÃ¢y dá»±ng mÃ´ hÃ¬nh Random Forest...")
    rf = RandomForestRegressor(featuresCol="scaled_features", labelCol="label", numTrees=50, maxDepth=10)
    pipeline = Pipeline(stages=[assembler, scaler, rf])
    return pipeline

# ğŸ¯ ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh
def evaluate_model(predictions):
    evaluator_rmse = RegressionEvaluator(labelCol="label", predictionCol="prediction", metricName="rmse")
    evaluator_r2 = RegressionEvaluator(labelCol="label", predictionCol="prediction", metricName="r2")

    rmse = evaluator_rmse.evaluate(predictions)
    r2 = evaluator_r2.evaluate(predictions)

    print(f"âœ… RMSE: {rmse}")
    print(f"ğŸ“Š RÂ² Score: {r2}")

# ğŸ” Hiá»ƒn thá»‹ káº¿t quáº£ dá»± Ä‘oÃ¡n
def analyze_predictions(predictions):
    print("\nğŸ“Š Má»™t sá»‘ káº¿t quáº£ dá»± Ä‘oÃ¡n:")
    predictions.select("Province_State", "label", "prediction").show(10)

    print("\nğŸš¨ Sai sá»‘ dá»± Ä‘oÃ¡n lá»›n nháº¥t:")
    predictions.withColumn("Prediction_Error", abs(col("label") - col("prediction"))) \
               .orderBy(col("Prediction_Error").desc()) \
               .select("Province_State", "label", "prediction", "Prediction_Error") \
               .show(10)

# ğŸ Cháº¡y quÃ¡ trÃ¬nh dá»± Ä‘oÃ¡n
def main():
    prepared_df = prepare_features(df)

    print("ğŸ“Š Chia táº­p dá»¯ liá»‡u thÃ nh Train/Test...")
    train_data, test_data = prepared_df.randomSplit([0.8, 0.2], seed=42)

    model_train, assembler, scaler = prepare_model_data(train_data)
    model_test, _, _ = prepare_model_data(test_data)

    pipeline = build_prediction_model(assembler, scaler)

    print("ğŸš€ Báº¯t Ä‘áº§u huáº¥n luyá»‡n mÃ´ hÃ¬nh...")
    model = pipeline.fit(model_train)

    print("ğŸ“¡ Thá»±c hiá»‡n dá»± Ä‘oÃ¡n trÃªn táº­p kiá»ƒm tra...")
    predictions = model.transform(model_test)

    print("ğŸ“ˆ ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh...")
    evaluate_model(predictions)

    print("ğŸ” PhÃ¢n tÃ­ch káº¿t quáº£ dá»± Ä‘oÃ¡n...")
    analyze_predictions(predictions)

    # â± Káº¿t thÃºc thá»i gian thá»±c thi
    end_time = time.time()
    print(f"â³ Thá»i gian thá»±c thi: {round(end_time - start_time, 2)} giÃ¢y")
    print(spark.sparkContext.uiWebUrl)
main()

# ğŸ›‘ ÄÃ³ng SparkSession
spark.stop()
